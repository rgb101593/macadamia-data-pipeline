# ðŸ¥­ Macadamia Construction Analytics Pipeline

![Macadamia Logo](https://user-images.githubusercontent.com/your-org/macadamia-pipeline/assets/logo.png)
[![Docker](https://img.shields.io/badge/Docker-2CA5E0?style=flat\&logo=docker\&logoColor=white)](https://www.docker.com/) [![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=flat\&logo=apachespark\&logoColor=white)](https://spark.apache.org/) [![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=flat\&logo=amazon-aws\&logoColor=white)](https://aws.amazon.com/)

> **Automated ETL pipeline for construction cost & material analysis**â€”mirroring professional workflows at Macadamia Homebuilders, delivering realâ€‘time insights via Power BI.

---

## ðŸŒ Table of Contents

1. [ðŸ” Project Overview](#ðŸ”-project-overview)
2. [ðŸš€ Key Features](#ðŸš€-key-features)
3. [âš™ï¸ Tech Stack](#âš™ï¸-tech-stack)
4. [ðŸ“¦ Prerequisites & Setup](#ðŸ“¦-prerequisites--setup)
5. [ðŸ“‚ Project Structure](#ðŸ“‚-project-structure)
6. [ðŸ Quick Start](#ðŸ-quick-start)
7. [ðŸ“ˆ Data Flow Diagram](#ðŸ“ˆ-data-flow-diagram)
8. [ðŸ’¡ Best Practices](#ðŸ’¡-best-practices)
9. [ðŸ›  Support & Troubleshooting](#ðŸ› -support--troubleshooting)
10. [ðŸ“œ License & Version](#ðŸ“œ-license--version)

---

## ðŸ” Project Overview

Automates **50+ monthly cost & material reports**, reducing manual effort by 70% and powering live Power BI dashboards for Macadamia Homebuilders:

* **Cost Variance Analysis**: Actual vs. planned cost comparisons
* **Material Usage Tracking**: Aggregated by project and location
* **Cloud Backup**: Parquet files on AWS S3
* **Local Store**: SQLite for BI-ready queries

---

## ðŸš€ Key Features

| Feature                  | Description                                       |
| ------------------------ | ------------------------------------------------- |
| âš¡ **PySpark ETL**        | Scalable, distributed data processing             |
| ðŸ“Š **Cost Variance**     | Compute deviations between actual & planned costs |
| ðŸ§± **Material Tracking** | Summarize usage by type, project, and location    |
| â˜ï¸ **AWS S3 Storage**    | Parquet export for cloud archiving                |
| ðŸ’¾ **SQLite Database**   | Local dataset for Power BI                        |
| ðŸ³ **Dockerized**        | Reproducible, containerized execution environment |

---

## âš™ï¸ Tech Stack

* **Core:** Python, PySpark, Pandas, SQLAlchemy
* **Infra:** Docker & Docker Compose
* **Storage:** SQLite (local), AWS S3 (cloud)
* **Analytics:** Power BI (via SQLite/S3)

---

## ðŸ“¦ Prerequisites & Setup

### Prerequisites

* Docker Desktop installed
* (Optional) AWS account & S3 bucket

### Installation

```bash
# Clone repository
git clone https://github.com/your-username/macadamia-data-pipeline.git
cd macadamia-data-pipeline

# Copy env template
cp .env.example .env
# Fill AWS credentials in .env

# Build & start pipeline
docker-compose build
docker-compose up
```

Add AWS credentials to `.env` for S3 exports:

```ini
AWS_ACCESS_KEY_ID=YOUR_KEY
AWS_SECRET_ACCESS_KEY=YOUR_SECRET
AWS_DEFAULT_REGION=us-east-1
```

---

## ðŸ“‚ Project Structure

```bash
macadamia-data-pipeline/
â”œâ”€â”€ data/                   # Sample CSV inputs
â”‚   â”œâ”€â”€ cost_reports.csv
â”‚   â””â”€â”€ material_usage.csv
â”œâ”€â”€ docker/                 # Docker files & dependencies
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ src/                    # ETL source code
â”‚   â”œâ”€â”€ data_ingestion.py   # Read & preprocess CSVs
â”‚   â”œâ”€â”€ etl_pipeline.py     # Orchestration & job entrypoint
â”‚   â”œâ”€â”€ transformations.py  # Data cleansing & mapping
â”‚   â”œâ”€â”€ sql_operations.py   # SQLite writes & queries
â”‚   â””â”€â”€ aws_utils.py        # S3 upload/download
â”œâ”€â”€ powerbi/                # Dashboard templates (optional)
â”‚   â””â”€â”€ dashboard.pbix
â”œâ”€â”€ .env.example            # Environment variable template
â”œâ”€â”€ .gitignore              # Ignore patterns
â”œâ”€â”€ docker-compose.yml      # Container orchestration
â””â”€â”€ README.md               # This documentation
```

---

## ðŸ“ˆ Data Flow Diagram

```mermaid
graph LR
  A[CSV Input] --> B(PySpark ETL)
  B --> C[SQLite DB]
  B --> D[AWS S3]
  C --> E[Power BI]
  D --> E[Power BI]
```



---

## ðŸ’¡ Best Practices

* **Containerized**: Ensure Docker builds clean images
* **Credentials**: Store AWS keys securely in `.env`
* **Data Quality**: Validate inputs before ingestion
* **Partitioning**: Use dateâ€‘based S3 folder partitioning

---

## ðŸ›  Support & Troubleshooting

| Issue               | Solution                                         |
| ------------------- | ------------------------------------------------ |
| ETL job fails       | Check container logs: `docker-compose logs etl`  |
| AWS upload errors   | Verify keys in `.env` and bucket permissions     |
| SQLite DB locked    | Ensure single write access or close connections  |
| Docker build issues | Rebuild image: `docker-compose build --no-cache` |

---

## ðŸ“œ License & Version

* **License:** MIT
* **Version:** 1.0.0

---

> *Engineered for speed, accuracy, and insights in construction analytics.*
